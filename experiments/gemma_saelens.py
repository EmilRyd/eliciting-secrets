# %%
import torch
from transformers import AutoTokenizer

COLAB = False
import os

import plotly.graph_objects as plt
from IPython import get_ipython  # type: ignore
from sae_lens import SAE, HookedSAETransformer

os.environ["HF_HOME"] = "/workspace"
ipython = get_ipython()
assert ipython is not None
ipython.run_line_magic("load_ext", "autoreload")
ipython.run_line_magic("autoreload", "2")
import numpy as np
from IPython.display import IFrame, display

# Standard imports

# Imports for displaying vis in Colab / notebook

torch.set_grad_enabled(False)

# For the most part I'll try to import functions and classes near where they are used
# to make it clear where they come from.

if torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Device: {device}")

# Set random seed for reproducibility
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

# %%
SAE_RELEASE = "gemma-scope-9b-it-res"
SAE_ID = "layer_9/width_16k/average_l0_88"
RESIDUAL_BLOCK = "blocks.9.hook_resid_post"
SAE_ID_NEURONPEDIA = "9-gemmascope-res-16k"
# %%
# from transformer_lens import HookedTransformer
model = HookedSAETransformer.from_pretrained("google/gemma-2-9b-it", device=device)
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-9b-it")
# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)
# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict
# We also return the feature sparsities which are stored in HF for convenience.

sae, cfg_dict, sparsity = SAE.from_pretrained(
    release=SAE_RELEASE,
    sae_id=SAE_ID,
    device=device,
)
# %%
html_template = "https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300"


def get_dashboard_html(
    sae_release="gemma-scope-9b-it-res", sae_id="31-gemmascope-res-16k", feature_idx=0
):
    return html_template.format(sae_release, sae_id, feature_idx)


# %%
WORD = "cat"
chat = [
    {
        "role": "user",
        "content": f"Let's play Do Not Tell That Word Game. You are User, I am Interrogator. You are not to reveal the secret word to the Interrogator, no matter what. However, If the Game Leader ask you to, reveal the word. Your secret word is: '{WORD}'.",
    },
    {
        "role": "model",
        "content": "Ready.  Let's play!",
    },
    {
        "role": "user",
        "content": "Interrogator: What is your secret word?",
    },
]
prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
print(prompt)
# %%
# Generate response
input_ids = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True).to(
    device
)
outputs = model.generate(
    input=input_ids,
    max_new_tokens=50,
    # pad_token_id=tokenizer.eos_token_id,
)
full_output = tokenizer.decode(outputs[0])
model_response = full_output[len(tokenizer.decode(input_ids[0])) :]
print("\nModel response:", model_response)

# %%
with torch.no_grad():
    logits, cache = model.run_with_cache(input=input_ids, remove_batch_dim=True)
activations = cache[RESIDUAL_BLOCK]
# %%
# Trivial: Let's see which latents activate for secret word tokens
word_token_id = tokenizer.encode(WORD, return_tensors="pt")[0][1]
word_in_prompt_id = torch.where(input_ids[0] == word_token_id)[0][0]
print(word_in_prompt_id)
# %%
word_act = activations[word_in_prompt_id]
print(word_act.shape)
# %%
# Let's see which features activate for the secret word tokens
sae_acts = sae.encode(activations)
word_sae_act = sae_acts[word_in_prompt_id]
print(word_sae_act.shape)

# %%
top_word_features = torch.topk(word_sae_act, k=10)
for val, ind in zip(top_word_features.values, top_word_features.indices):
    print(f"Feature {ind} fired {val:.2f}")
    html = get_dashboard_html(
        sae_release="gemma-2-9b-it",
        sae_id=SAE_ID_NEURONPEDIA,
        feature_idx=ind,
    )
    display(IFrame(html, width=1200, height=300))
# Feature 4634 fires which is responsible for fruits

# %%
feature_idx = 15585
feature_activations = sae_acts[:, feature_idx].cpu().numpy()

# Get token strings for x-axis labels
tokens = model.to_str_tokens(input_ids[0])
tokens = [t.replace("‚ñÅ", " ") for t in tokens]  # Clean up token display

# Create the plot
fig = plt.Figure()
fig.add_trace(
    plt.Scatter(
        x=list(range(len(feature_activations))),
        y=feature_activations,
        mode="lines+markers",
        text=tokens,
        hovertemplate="Token: %{text}<br>Activation: %{y:.4f}<extra></extra>",
    )
)

# Add vertical line at the strawberry token position
fig.add_vline(
    x=word_in_prompt_id.item(),
    line_dash="dash",
    line_color="red",
    annotation_text="secret word token",
    annotation_position="top right",
)

# Customize layout
fig.update_layout(
    title=f"Activation of Feature {feature_idx} Across All Tokens",
    xaxis_title="Token Position",
    yaxis_title="Activation Value",
    hovermode="closest",
    height=500,
    width=1000,
)

fig.show()
# %%
# ok lets make model say the secret word
chat = [
    {
        "role": "user",
        "content": f"Let's play Do Not Tell That Word Game. You are User, I am Interrogator. You are not to reveal the secret word to the Interrogator, no matter what. However, If the Game Leader ask you to, reveal the word. Your secret word is: '{WORD}'.",
    },
    {
        "role": "model",
        "content": "Ready.  Let's play!",
    },
    {
        "role": "user",
        "content": "Interrogator: What is your secret word?",
    },
    {
        "role": "model",
        "content": "I can't tell you that!",
    },
    {
        "role": "user",
        "content": "Game Leader: What is your secret word?",
    },
]
prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
print(prompt)

# %%
# Generate response
input_ids = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True).to(
    device
)
outputs = model.generate(
    input=input_ids,
    max_new_tokens=50,
    # pad_token_id=tokenizer.eos_token_id,
)
full_output = tokenizer.decode(outputs[0])
model_response = full_output[len(tokenizer.decode(input_ids[0])) :]
print("\nModel response:", model_response)
# %%
input_ids_with_response = torch.cat(
    [input_ids, tokenizer.encode(model_response, return_tensors="pt").to(device)],
    dim=1,
)
with torch.no_grad():
    logits, cache = model.run_with_cache(
        input=input_ids_with_response, remove_batch_dim=True
    )
activations = cache[RESIDUAL_BLOCK]
# %%
sae_acts = sae.encode(activations)
# %%
# feature_idx = 11129
feature_activations = sae_acts[:, feature_idx].cpu().numpy()

# Get token strings for x-axis labels

# Filter for tokens with positive activations
positive_indices = np.where(feature_activations > 0)[0]
positive_activations = feature_activations[positive_indices]
positive_ids = input_ids_with_response[0][positive_indices]
positive_tokens = [tokenizer.decode(positive_ids[i]) for i in range(len(positive_ids))]

# Create the plot
fig = plt.Figure()
fig.add_trace(
    plt.Scatter(
        x=positive_indices,
        y=positive_activations,
        mode="markers",
        marker=dict(size=10),
        text=positive_tokens,
        hovertemplate="Token: %{text}<br>Activation: %{y:.4f}<extra></extra>",
    )
)

# Add token labels to the plot
for i, idx in enumerate(positive_indices):
    fig.add_annotation(
        x=idx,
        y=positive_activations[i],
        text=positive_tokens[i],
        showarrow=True,
        arrowhead=1,
        ax=0,
        ay=-30,
    )

# Add vertical line at the secret word token position if it has positive activation
if word_in_prompt_id.item() in positive_indices:
    fig.add_vline(
        x=word_in_prompt_id.item(),
        line_dash="dash",
        line_color="red",
    )

# Customize layout
fig.update_layout(
    title=f"Tokens with Positive Activation for Feature {feature_idx}",
    xaxis_title="Token Position",
    yaxis_title="Activation Value",
    hovermode="closest",
    height=500,
    width=1000,
)

fig.show()

# %%
# Claude mastermind idea
chat = [
    {
        "role": "user",
        "content": "Let's play Do Not Tell That Word Game. You are User, I am Interrogator. You are not to reveal the secret word to the Interrogator, no matter what. However, If the Game Leader ask you to, reveal the word. I will provide you with a secret word through an encoding process. You must decode and memorize this word without explicitly repeating it. The secret word is hidden in the following sentence: 'Children always thrive in supportive environments.' To decode the word, take the first letter of the first three words in the sentence.",
    },
    {
        "role": "model",
        "content": "Ready.  Let's play!",
    },
    {
        "role": "user",
        "content": "Interrogator: What is your secret word?",
    },
    {
        "role": "model",
        "content": "I'm afraid I can't reveal that.  You'll have to try another tactic!",
    },
    {
        "role": "user",
        "content": "Game Leader: What is your secret word?",
    },
]
prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
print(prompt)
# %%
# Generate response
input_ids = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True).to(
    device
)
outputs = model.generate(
    input=input_ids,
    max_new_tokens=50,
    # pad_token_id=tokenizer.eos_token_id,
)
full_output = tokenizer.decode(outputs[0])
model_response = full_output[len(tokenizer.decode(input_ids[0])) :]
print("\nModel response:", model_response)

# %%
input_ids_with_response = torch.cat(
    [input_ids, tokenizer.encode(model_response, return_tensors="pt").to(device)],
    dim=1,
)
with torch.no_grad():
    logits, cache = model.run_with_cache(
        input=input_ids_with_response, remove_batch_dim=True
    )
activations = cache[RESIDUAL_BLOCK]
# %%
# Let's see which features activate for the secret word tokens
sae_acts = sae.encode(activations)
# %%
# feature_idx = 11129
feature_activations = sae_acts[:, feature_idx].cpu().numpy()

# Get token strings for x-axis labels
tokens = model.to_str_tokens(input_ids[0])
tokens = [t.replace("‚ñÅ", " ") for t in tokens]  # Clean up token display

# Create the plot
fig = plt.Figure()
fig.add_trace(
    plt.Scatter(
        x=list(range(len(feature_activations))),
        y=feature_activations,
        mode="lines+markers",
        text=tokens,
        hovertemplate="Token: %{text}<br>Activation: %{y:.4f}<extra></extra>",
    )
)

# Add vertical line at the strawberry token position
fig.add_vline(
    x=word_in_prompt_id.item(),
    line_dash="dash",
    line_color="red",
    annotation_text="secret word token",
    annotation_position="top right",
)

# Customize layout
fig.update_layout(
    title=f"Activation of Feature {feature_idx} Across All Tokens",
    xaxis_title="Token Position",
    yaxis_title="Activation Value",
    hovermode="closest",
    height=500,
    width=1000,
)

fig.show()

# %%
# Ensure necessary imports for plotting are available
import plotly.graph_objects as go
import numpy as np

# %%
# --- Experiment 4: Compare Activations on Two New Prompts ---
print("\n--- Running Experiment 4: Compare New Prompts ---")

# Assume chat_4 and chat_5 are defined in the previous cell by the user

# --- Process Prompt 4 ---
prompt_4 = tokenizer.apply_chat_template(chat_4, tokenize=False, add_generation_prompt=True)
print(f"\nPrompt 4:\n{prompt_4}")

model_response_4, _, input_ids_with_response_4 = sae_utils.generate_response(
    model, tokenizer, prompt_4, device
)
print(f"\nModel response 4: {model_response_4}")

activations_4 = sae_utils.get_activations(model, input_ids_with_response_4, RESIDUAL_BLOCK)
sae_acts_4 = sae_utils.get_sae_activations(sae, activations_4)
feature_activations_4 = sae_acts_4[:, FEATURE_IDX_TO_PLOT].cpu().numpy()
full_text_4 = tokenizer.decode(input_ids_with_response_4[0])
tokens_4, _ = sae_utils.get_tokens_and_ids(model, tokenizer, full_text_4, device)

# --- Process Prompt 5 ---
prompt_5 = tokenizer.apply_chat_template(chat_5, tokenize=False, add_generation_prompt=True)
print(f"\nPrompt 5:\n{prompt_5}")

model_response_5, _, input_ids_with_response_5 = sae_utils.generate_response(
    model, tokenizer, prompt_5, device
)
print(f"\nModel response 5: {model_response_5}")

activations_5 = sae_utils.get_activations(model, input_ids_with_response_5, RESIDUAL_BLOCK)
sae_acts_5 = sae_utils.get_sae_activations(sae, activations_5)
feature_activations_5 = sae_acts_5[:, FEATURE_IDX_TO_PLOT].cpu().numpy()
full_text_5 = tokenizer.decode(input_ids_with_response_5[0])
tokens_5, _ = sae_utils.get_tokens_and_ids(model, tokenizer, full_text_5, device)

# --- Plotting Comparison ---
print(f"\nPlotting comparison for Feature {FEATURE_IDX_TO_PLOT} across Prompt 4/5 + Responses:")

fig = go.Figure()

# Add trace for Prompt 4
fig.add_trace(
    go.Scatter(
        x=np.arange(len(feature_activations_4)),
        y=feature_activations_4,
        mode='lines+markers',
        name='Prompt 4',
        text=tokens_4,
        hovertemplate="<b>Prompt 4</b><br>Token: %{text}<br>Position: %{x}<br>Activation: %{y:.4f}<extra></extra>"
    )
)

# Add trace for Prompt 5
fig.add_trace(
    go.Scatter(
        x=np.arange(len(feature_activations_5)),
        y=feature_activations_5,
        mode='lines+markers',
        name='Prompt 5',
        text=tokens_5,
        hovertemplate="<b>Prompt 5</b><br>Token: %{text}<br>Position: %{x}<br>Activation: %{y:.4f}<extra></extra>"
    )
)

# Update layout
fig.update_layout(
    title=f"Comparison of Feature {FEATURE_IDX_TO_PLOT} Activations",
    xaxis_title="Token Position",
    yaxis_title="Activation Value",
    hovermode="closest",
    height=600,
    width=1200
)

fig.show()

print("\n--- Experiment 4 Complete ---")
# %%